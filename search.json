[
  {
    "objectID": "base 2d object segmentation.html",
    "href": "base 2d object segmentation.html",
    "title": "Base - 2D object segmentation",
    "section": "",
    "text": "SUN RGB-D -\nnuscenes -\nImagenet -"
  },
  {
    "objectID": "base 2d object segmentation.html#datasets",
    "href": "base 2d object segmentation.html#datasets",
    "title": "Base - 2D object segmentation",
    "section": "",
    "text": "SUN RGB-D -\nnuscenes -\nImagenet -"
  },
  {
    "objectID": "base 2d object segmentation.html#models",
    "href": "base 2d object segmentation.html#models",
    "title": "Base - 2D object segmentation",
    "section": "Models",
    "text": "Models\ndetectron2 - https://ai.meta.com/tools/detectron2/\nYolo"
  },
  {
    "objectID": "base 2d object segmentation.html#need",
    "href": "base 2d object segmentation.html#need",
    "title": "Base - 2D object segmentation",
    "section": "Need",
    "text": "Need\n\ndataset with labeled object segmentations"
  },
  {
    "objectID": "base 2d object segmentation.html#detectron---facebook",
    "href": "base 2d object segmentation.html#detectron---facebook",
    "title": "Base - 2D object segmentation",
    "section": "Detectron - Facebook",
    "text": "Detectron - Facebook\n\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\n\npath = \"./Data/data_depth_selection/depth_selection/test_depth_completion_anonymous/image/0000000000.png\"\n\nimage = Image.open(path)\nim = image\nim.show()  # show image\nim.save('original.jpg')  # save image\n\n\n\n\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/transformers/models/detr/image_processing_detr.py:773: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n  warnings.warn(\n\n\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let's only keep detections with score &gt; 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\nimg = np.array(image)\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n            f\"{round(score.item(), 3)} at location {box}\"\n    )\n\n    start_point = (int(box[0]), int(box[1]))\n    end_point = (int(box[2]), int(box[3]))\n    color = (0, 255, 0)  # Green color\n    thickness = 2\n\n    # Draw the bounding box\n    img = cv2.rectangle(img, start_point, end_point, color, thickness)\n\nim = Image.fromarray(img)  # RGB PIL image\nim.show()  # show image\n\n\n\n\nDetected car with confidence 0.998 at location [817.33, 166.45, 1060.32, 259.68]\nDetected car with confidence 0.986 at location [667.85, 159.86, 758.38, 210.69]\nDetected car with confidence 0.993 at location [395.82, 170.7, 467.47, 217.21]\nDetected car with confidence 0.992 at location [1141.6, 173.02, 1215.77, 349.43]\nDetected person with confidence 0.967 at location [994.13, 156.7, 1013.04, 182.61]\nDetected car with confidence 0.956 at location [627.74, 152.47, 670.72, 178.78]\nDetected car with confidence 0.935 at location [453.03, 171.88, 485.05, 200.99]\nDetected car with confidence 0.992 at location [245.5, 181.43, 384.24, 270.18]\nDetected person with confidence 0.974 at location [17.56, 166.55, 56.18, 207.64]\nDetected car with confidence 0.998 at location [0.79, 192.15, 252.53, 348.11]\nDetected car with confidence 0.954 at location [662.21, 155.05, 737.88, 187.69]\nDetected car with confidence 0.989 at location [752.67, 157.03, 878.92, 239.88]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom transformers import DetrFeatureExtractor, DetrForSegmentation\nfrom PIL import Image\nimport requests\n\n\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-dc5-panoptic')\nmodel = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-dc5-panoptic')\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/transformers/models/detr/image_processing_detr.py:773: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n  warnings.warn(\n\n\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n# model predicts COCO classes, bounding boxes, and masks\n\n\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\nmasks = outputs.pred_masks\n\n\nimport matplotlib.pyplot as plt\n\n# Get the predicted segmentation masks\nsegmentation_masks = outputs.pred_masks.detach().numpy()\n\n# Remove the channel dimension (C) to make it 2D\nsegmentation_masks1 = segmentation_masks[0, 1]\nsegmentation_masks2 = segmentation_masks[0, 2]\nsegmentation_masks3 = segmentation_masks[0, 3]\n\n# Display the segmentation masks\nplt.imshow(segmentation_masks[0, 5], cmap='viridis')\n\nplt.show()\n\n\n\n\n\nplt.imshow(segmentation_masks3, cmap='viridis')\nplt.show()\n\n\n\n\n\n# Display the segmentation masks\nplt.imshow(segmentation_masks[0, 10], cmap='viridis')\nplt.show()"
  },
  {
    "objectID": "base 2d object segmentation.html#yolo",
    "href": "base 2d object segmentation.html#yolo",
    "title": "Base - 2D object segmentation",
    "section": "YOLO",
    "text": "YOLO\n\nfrom transformers import YolosImageProcessor, YolosForObjectDetection\nfrom PIL import Image\nimport torch\nimport requests\n\n\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\nimage_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/transformers/models/yolos/image_processing_yolos.py:704: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n  warnings.warn(\n\n\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n\n\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\nimg = np.array(image)\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )\n    start_point = (int(box[0]), int(box[1]))\n    end_point = (int(box[2]), int(box[3]))\n    color = (0, 255, 0)  # Green color\n    thickness = 2\n\n    # Draw the bounding box\n    img = cv2.rectangle(img, start_point, end_point, color, thickness)\n\nim = Image.fromarray(img)  # RGB PIL image\nim.show()  # show image\nim.save('resultsdetection.jpg')  # save image\n\n\n\n\nDetected person with confidence 0.934 at location [10.64, 168.62, 58.12, 205.55]\nDetected car with confidence 0.96 at location [643.33, 154.64, 752.81, 177.49]\nDetected car with confidence 0.938 at location [756.49, 169.97, 1079.44, 252.74]\nDetected car with confidence 0.989 at location [376.94, 173.83, 482.96, 215.65]\nDetected car with confidence 0.904 at location [220.8, 189.15, 413.8, 262.7]\nDetected car with confidence 0.907 at location [650.67, 156.56, 756.42, 179.22]\nDetected car with confidence 0.959 at location [665.38, 165.02, 824.25, 208.54]\nDetected car with confidence 0.977 at location [0.21, 202.67, 262.33, 324.33]\nDetected car with confidence 0.951 at location [1139.86, 183.39, 1215.81, 336.22]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom transformers import YolosFeatureExtractor, YolosForObjectDetection\nfrom PIL import Image\nimport requests\n\n\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\n\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n\n\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\nimg = np.array(image)\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )\n    start_point = (int(box[0]), int(box[1]))\n    end_point = (int(box[2]), int(box[3]))\n    color = (0, 255, 0)  # Green color\n    thickness = 2\n\n    # Draw the bounding box\n    img = cv2.rectangle(img, start_point, end_point, color, thickness)\nim = Image.fromarray(img)  # RGB PIL image\nim.show()  # show image\n\n\n\n\nDetected car with confidence 0.996 at location [398.04, 173.37, 479.42, 216.12]\nDetected car with confidence 0.973 at location [1142.69, 179.11, 1215.77, 344.98]\nDetected car with confidence 0.939 at location [522.86, 161.63, 551.34, 173.99]\nDetected person with confidence 0.984 at location [14.35, 164.78, 55.11, 207.47]\nDetected car with confidence 0.996 at location [811.19, 166.15, 1064.9, 258.93]\nDetected car with confidence 0.999 at location [0.33, 196.74, 256.94, 349.14]\nDetected car with confidence 0.925 at location [690.92, 159.53, 895.01, 234.39]\nDetected car with confidence 0.943 at location [635.74, 153.3, 704.57, 180.68]\nDetected car with confidence 0.999 at location [253.67, 181.78, 399.16, 271.7]\nDetected car with confidence 0.983 at location [680.44, 163.26, 768.36, 205.59]\nDetected car with confidence 0.971 at location [666.17, 156.34, 741.3, 180.36]"
  },
  {
    "objectID": "base 2d object segmentation.html#yolo-v8",
    "href": "base 2d object segmentation.html#yolo-v8",
    "title": "Base - 2D object segmentation",
    "section": "Yolo v8",
    "text": "Yolo v8\n\nfrom PIL import Image\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLOv8n model\nmodel = YOLO('yolov8n.pt')\n\n\n# Run inference on 'bus.jpg'\nresults = model(image)  # results list\n\n# Show the results\nfor r in results:\n    im_array = r.plot()  # plot a BGR numpy array of predictions\n    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n    im.show()  # show image\n    im.save('resultsdetection1.jpg')  # save image\n\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n  return F.conv2d(input, weight, bias, self.stride,\n0: 192x640 9 cars, 61.4ms\nSpeed: 7.1ms preprocess, 61.4ms inference, 12.8ms postprocess per image at shape (1, 3, 192, 640)\n\n\n\n\n\n\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLOv8n-seg Segment model\nmodel = YOLO('yolov8n-seg.pt')\n\n# Run inference on an image\nresults = model(image)  # results list\n\n# View results\nfor r in results:\n    im_array = r.plot()  # plot a BGR numpy array of predictions\n    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n    im.show()  # show image\n    im.save('resultsseg.jpg')  # save image\n\n\n0: 192x640 1 person, 7 cars, 1 truck, 20.5ms\nSpeed: 1.2ms preprocess, 20.5ms inference, 3.3ms postprocess per image at shape (1, 3, 192, 640)"
  },
  {
    "objectID": "base 2d object segmentation.html#segformer",
    "href": "base 2d object segmentation.html#segformer",
    "title": "Base - 2D object segmentation",
    "section": "Segformer",
    "text": "Segformer\n\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\n\nprocessor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)"
  },
  {
    "objectID": "base 2d object segmentation.html#maskformerimageprocessor",
    "href": "base 2d object segmentation.html#maskformerimageprocessor",
    "title": "Base - 2D object segmentation",
    "section": "MaskFormerImageProcessor",
    "text": "MaskFormerImageProcessor\n\nfrom transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\n\n# load MaskFormer fine-tuned on COCO panoptic segmentation\nprocessor = MaskFormerImageProcessor.from_pretrained(\"facebook/maskformer-swin-large-coco\")\nmodel = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-large-coco\")\n\n\ninputs = processor(images=image, return_tensors=\"pt\")\n\n\noutputs = model(**inputs)\n# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n\n\nclass_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\n\n# you can pass them to processor for postprocessing\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n# we refer to the demo notebooks for visualization (see \"Resources\" section in the MaskFormer docs)\npredicted_panoptic_map = result[\"segmentation\"]"
  },
  {
    "objectID": "seminar.html",
    "href": "seminar.html",
    "title": "Seminar",
    "section": "",
    "text": "30% Mastery of materials, excellent progress, originality\n30% Good insights, good converage of background research, good flow of ideas\nConfident, good eye contact, clear and articular, good answers\nVisuals are clear, uncluttered, relevent, etc.\nPacing is appropriate, seminar stays within time."
  },
  {
    "objectID": "seminar.html#criteria",
    "href": "seminar.html#criteria",
    "title": "Seminar",
    "section": "",
    "text": "30% Mastery of materials, excellent progress, originality\n30% Good insights, good converage of background research, good flow of ideas\nConfident, good eye contact, clear and articular, good answers\nVisuals are clear, uncluttered, relevent, etc.\nPacing is appropriate, seminar stays within time."
  },
  {
    "objectID": "seminar.html#activity-1",
    "href": "seminar.html#activity-1",
    "title": "Seminar",
    "section": "Activity 1",
    "text": "Activity 1\n\nSummarize what I need to achieve by the end of this semester.\n\nCreate a base line depth estimator\nCreate a base line 2D object segmentation model\nAdd depth erstimator to the 2D object segmentation model\n\nWhat have you achieved so far?\n\nCreate a base line depth estimator\nCreate a base line 2D object segmentation model\n\nWhat is left to be done?\n\nAdd depth erstimator to the 2D object segmentation model"
  },
  {
    "objectID": "seminar.html#seminar-preparation-logistics",
    "href": "seminar.html#seminar-preparation-logistics",
    "title": "Seminar",
    "section": "Seminar Preparation Logistics",
    "text": "Seminar Preparation Logistics\n\nDate - how much time to prepare\nTime allowed for the talk - 15 mins + 5 mins Qs\nDoes that include time for questions\nRoom - how big, what shape?\nRoom/Zoom equipement\nYour equipment - laptop\nAudience - who are they, how much do they know already\nOpportunity for technical rehearsal?"
  },
  {
    "objectID": "seminar.html#first-steps",
    "href": "seminar.html#first-steps",
    "title": "Seminar",
    "section": "First steps",
    "text": "First steps\n\nPurpose\n\nAudience\n\nSchool, research division\nShare some context\n\nEffect - Accept thesis/ project proposal\nMessage\n\nWhy is this problem interesting?\nHow am i going to solve it?\nWhat do i want them to remember?\n\n\nStructure\n\nwhat to say\n\nPresentation\n\nHow to deliver it?"
  },
  {
    "objectID": "seminar.html#structure",
    "href": "seminar.html#structure",
    "title": "Seminar",
    "section": "Structure",
    "text": "Structure\n\nOutline - Outline of presentation\nBody - Then you them\nSummary\n\nTime line on the side"
  },
  {
    "objectID": "seminar.html#story-board",
    "href": "seminar.html#story-board",
    "title": "Seminar",
    "section": "Story Board",
    "text": "Story Board\n\nintroduce the topic\n\n\ntitle\nname\n\n\ndefinition of topic\n\n\nhigh level understanding (eyes) with diagram\nWell, how can we bring that to computer vision\n\n\nLiterature review and previous results\n\n\nIs that obvious?\ndefine monocular\ndefine 2.5D\ndefine object classification, object detection and object segmentation\nLets dig a little deeper - current default method in object detection and segmenation\nResearch into Depth estimators from monocular input\n\nmodels\ndataset\n\nResearch into 2.5 object classification, object detection and object segmentation\n\nmodels\ndataset\n\nGap in knowledge\nmy specific project question/topic\nMy method to address my question\nChallenges i face\n\nMeasure compute complexity\nMeasure time complexity\n\nProgress so far\nPossible outcomes\nSignificance of this research\nFuture work and take home message\ndiagram\nsignificance of this research\ncontents"
  },
  {
    "objectID": "depth estimator.html",
    "href": "depth estimator.html",
    "title": "Monocular Depth Estimator",
    "section": "",
    "text": "detectron2 - https://ai.meta.com/tools/detectron2/"
  },
  {
    "objectID": "depth estimator.html#models",
    "href": "depth estimator.html#models",
    "title": "Monocular Depth Estimator",
    "section": "",
    "text": "detectron2 - https://ai.meta.com/tools/detectron2/"
  },
  {
    "objectID": "depth estimator.html#datasets",
    "href": "depth estimator.html#datasets",
    "title": "Monocular Depth Estimator",
    "section": "Datasets",
    "text": "Datasets\nnuscenes -\nImagenet -\nNYU Depth Dataset V2 Kitti Sun rgb-d"
  },
  {
    "objectID": "depth estimator.html#glpn",
    "href": "depth estimator.html#glpn",
    "title": "Monocular Depth Estimator",
    "section": "GLPN",
    "text": "GLPN\nThe GLPN model was proposed in Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth\n\nfrom transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\n\n\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(\"vinvino02/glpn-kitti\")\nmodel = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-kitti\")\n\n\npath = \"./Data/data_depth_selection/depth_selection/test_depth_completion_anonymous/image/0000000000.png\"\n\nimage = Image.open(path)\nimage\n\n\n\n\n\n# prepare image for the model\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_depth = outputs.predicted_depth\n\n\n# interpolate to original size\nprediction = torch.nn.functional.interpolate(\n    predicted_depth.unsqueeze(1),\n    size=image.size[::-1],\n    mode=\"bicubic\",\n    align_corners=False,\n)\n\n\n# visualize the prediction\noutput = prediction.squeeze().cpu().numpy()\nplt.imshow(output)\nplt.savefig(\"depth1png\")\n\n\n\n\n\nformatted = (output * 255 / np.max(output)).astype(\"uint8\")\nim = Image.fromarray(formatted)\nim.show()  # show image\nim.save('depth.jpg')  # save image"
  },
  {
    "objectID": "depth estimator.html#midas",
    "href": "depth estimator.html#midas",
    "title": "Monocular Depth Estimator",
    "section": "Midas",
    "text": "Midas\n\nimport cv2\nimport torch\nimport urllib.request\n\nimport matplotlib.pyplot as plt\n\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\nurllib.request.urlretrieve(url, filename)\n\n('dog.jpg', &lt;http.client.HTTPMessage&gt;)\n\n\n\nmodel_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n\nmidas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n\nUsing cache found in /home/ben/.cache/torch/hub/intel-isl_MiDaS_master\n\n\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\nmidas.to(device)\nmidas.eval()\n\nDPTDepthModel(\n  (pretrained): Module(\n    (model): VisionTransformer(\n      (patch_embed): PatchEmbed(\n        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n        (norm): Identity()\n      )\n      (pos_drop): Dropout(p=0.0, inplace=False)\n      (patch_drop): Identity()\n      (norm_pre): Identity()\n      (blocks): Sequential(\n        (0): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (1): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (2): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (3): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (4): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (5): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (6): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (7): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (8): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (9): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (10): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (11): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (12): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (13): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (14): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (15): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (16): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (17): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (18): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (19): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (20): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (21): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (22): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n        (23): Block(\n          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (attn): Attention(\n            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n            (q_norm): Identity()\n            (k_norm): Identity()\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n          (ls1): Identity()\n          (drop_path1): Identity()\n          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (ls2): Identity()\n          (drop_path2): Identity()\n        )\n      )\n      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n      (fc_norm): Identity()\n      (head_drop): Dropout(p=0.0, inplace=False)\n      (head): Linear(in_features=1024, out_features=1000, bias=True)\n    )\n    (act_postprocess1): Sequential(\n      (0): ProjectReadout(\n        (project): Sequential(\n          (0): Linear(in_features=2048, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n        )\n      )\n      (1): Transpose()\n      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n    )\n    (act_postprocess2): Sequential(\n      (0): ProjectReadout(\n        (project): Sequential(\n          (0): Linear(in_features=2048, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n        )\n      )\n      (1): Transpose()\n      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (act_postprocess3): Sequential(\n      (0): ProjectReadout(\n        (project): Sequential(\n          (0): Linear(in_features=2048, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n        )\n      )\n      (1): Transpose()\n      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (act_postprocess4): Sequential(\n      (0): ProjectReadout(\n        (project): Sequential(\n          (0): Linear(in_features=2048, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n        )\n      )\n      (1): Transpose()\n      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n  )\n  (scratch): Module(\n    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (refinenet1): FeatureFusionBlock_custom(\n      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (resConfUnit1): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (resConfUnit2): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n    )\n    (refinenet2): FeatureFusionBlock_custom(\n      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (resConfUnit1): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (resConfUnit2): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n    )\n    (refinenet3): FeatureFusionBlock_custom(\n      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (resConfUnit1): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (resConfUnit2): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n    )\n    (refinenet4): FeatureFusionBlock_custom(\n      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (resConfUnit1): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (resConfUnit2): ResidualConvUnit_custom(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (activation): ReLU()\n        (skip_add): FloatFunctional(\n          (activation_post_process): Identity()\n        )\n      )\n      (skip_add): FloatFunctional(\n        (activation_post_process): Identity()\n      )\n    )\n    (output_conv): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): Interpolate()\n      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU(inplace=True)\n      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n      (5): ReLU(inplace=True)\n      (6): Identity()\n    )\n  )\n)\n\n\n\nmidas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n\nif model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n    transform = midas_transforms.dpt_transform\nelse:\n    transform = midas_transforms.small_transform\n\nUsing cache found in /home/ben/.cache/torch/hub/intel-isl_MiDaS_master\n\n\n\nfilename = path\n\n\nimg = cv2.imread(filename)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\ninput_batch = transform(img).to(device)\n\n\nwith torch.no_grad():\n    prediction = midas(input_batch)\n\n    prediction = torch.nn.functional.interpolate(\n        prediction.unsqueeze(1),\n        size=img.shape[:2],\n        mode=\"bicubic\",\n        align_corners=False,\n    ).squeeze()\n\noutput1 = prediction.cpu().numpy()\n\n\nplt.imshow(output1)\nplt.savefig(\"depth2.png\")\n\n# plt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "REIT4842_notes",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "REIT4842_notes",
    "section": "Install",
    "text": "Install\npip install REIT4842_notes"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "REIT4842_notes",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "research_2d->3d.html",
    "href": "research_2d->3d.html",
    "title": "Research 2D to 2.5D",
    "section": "",
    "text": "(pdf) Monocular depth estimation based on deep learning: An overview - https://link.springer.com/article/10.1007/s11431-020-1582-8\nMonocular Depth Estimation: A Survey - https://arxiv.org/pdf/1901.09402.pdf\nDeep learning for monocular depth estimation: A review - https://www.sciencedirect.com/science/article/pii/S0925231220320014?casa_token=ysp2u82qtI4AAAAA:cUbSkX06F0sADtiP6Gc-d27w4jpMdqGrd4fpZl88H1Vm4z5UBzBkmi5AJ5SMhwYliEhzrOsh6Q"
  },
  {
    "objectID": "research_2d->3d.html#papers",
    "href": "research_2d->3d.html#papers",
    "title": "Research 2D to 2.5D",
    "section": "",
    "text": "(pdf) Monocular depth estimation based on deep learning: An overview - https://link.springer.com/article/10.1007/s11431-020-1582-8\nMonocular Depth Estimation: A Survey - https://arxiv.org/pdf/1901.09402.pdf\nDeep learning for monocular depth estimation: A review - https://www.sciencedirect.com/science/article/pii/S0925231220320014?casa_token=ysp2u82qtI4AAAAA:cUbSkX06F0sADtiP6Gc-d27w4jpMdqGrd4fpZl88H1Vm4z5UBzBkmi5AJ5SMhwYliEhzrOsh6Q"
  },
  {
    "objectID": "research_2d->3d.html#timeline",
    "href": "research_2d->3d.html#timeline",
    "title": "Research 2D to 2.5D",
    "section": "Timeline",
    "text": "Timeline"
  },
  {
    "objectID": "research_2d->3d.html#datasets",
    "href": "research_2d->3d.html#datasets",
    "title": "Research 2D to 2.5D",
    "section": "Datasets",
    "text": "Datasets\n\nNYUD-V2\nKITTI\nMake3D\nSUN RGB-D -"
  },
  {
    "objectID": "research_2d->3d.html#models",
    "href": "research_2d->3d.html#models",
    "title": "Research 2D to 2.5D",
    "section": "Models",
    "text": "Models\n\nuse FFT"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Proposal",
    "section": "",
    "text": "Criteria\nPass\nFail\n\n\n\n\nTopic Definition\nClear and concise definition of the research question or problem to be addressed, along with a compelling motivation for why it is important. The topic should be relevant to the field of study and demonstrate originality and potential for contribution.\nUnclear or vague research question or problem, with no clear motivation or relevance to the field of study. The topic may be too broad, too narrow, or unoriginal.\n\n\nAims\nThe aims are clearly stated and focused on the overall goals and objectives of the research project and demonstrate a clear understanding of the research problem and its significance.\nThe aims are vague or unclear, making it difficult to determine the purpose of the research project, or the aims are task-focused rather than outcome-focused, indicating a lack of understanding of the research problem and its significance.\n\n\nBackground\nComprehensive and well-organized review of the relevant background material, providing a good background to understand the topic are presented.\nSuperficial or incomplete review of the background material, poorly organized, or demonstrates a poor understanding of the topic area\n\n\nLiterature Review\nComprehensive and critical review of the literature related to the research question or problem, demonstrating a strong understanding of the existing research, identifying gaps and limitations.\nIncomplete or superficial review of the literature, lacking in critical analysis or coherence, and failing to identify relevant gaps or limitations in the existing research. The review may also be poorly organized.\n\n\nSelection of references\nAt least 15 references are presented from reliable sources which have demonstrated relevance to the proposed project.\nSome references are unreliable or irrelevant to the proposed project, or there are too few of them.\n\n\nMilestones\nWell thought out milestones, with the associated due dates as well as required resources have been stated. (This section can be brief in the draft version)\nMilestones are not well thought out, and not in logical flow, or no associated dates or resources have been identified.\n\n\nBibliographic style\nThe references conform to the IEEE or APA style.\nSome references are incorrectly formatted."
  },
  {
    "objectID": "proposal.html#single-camera-2.5d-image",
    "href": "proposal.html#single-camera-2.5d-image",
    "title": "Proposal",
    "section": "Single Camera –> 2.5D image",
    "text": "Single Camera –&gt; 2.5D image\n\nuse 2.5D image to train 2.5D deep learning model\ndo DFT to test for improvements"
  },
  {
    "objectID": "proposal.html#d-object-recognition",
    "href": "proposal.html#d-object-recognition",
    "title": "Proposal",
    "section": "2.5D –> object recognition",
    "text": "2.5D –&gt; object recognition\n\nVideo calls - looking glass"
  },
  {
    "objectID": "proposal.html#to-do",
    "href": "proposal.html#to-do",
    "title": "Proposal",
    "section": "To Do:",
    "text": "To Do:\n\nimage to FFT\nimage to depth\nimage + FFT -&gt; depth\nimage + FFT + depth -&gt; segmentation"
  },
  {
    "objectID": "research_object_segmentation.html",
    "href": "research_object_segmentation.html",
    "title": "Research Object Segmentation and Object detection",
    "section": "",
    "text": "(pdf) High-quality indoor scene 3D reconstruction with RGB-D cameras: A brief review - https://link.springer.com/article/10.1007/s41095-021-0250-8\n(pdf) Deep Learning for Generic Object Detection: A Survey - https://link.springer.com/article/10.1007/s11263-019-01247-4\nProgress in multi-object detection models: a comprehensive survey - https://link.springer.com/article/10.1007/s11042-022-14131-0\nDepth-Assisted Joint Detection Network For Monocular 3d Object Detection - https://ieeexplore.ieee.org/document/9506647\nMonocular 3D Object Detection of Moving Objects Using Random Sampling and Deep Layer Aggregation - https://ieeexplore.ieee.org/document/10043422\nDepth-Assisted Joint Detection Network For Monocular 3d Object Detection - https://ieeexplore.ieee.org/document/9966379\nCross-Modality Knowledge Distillation Network for Monocular 3D Object Detection - https://arxiv.org/pdf/2211.07171v1.pdf\n3D Object Detection - https://paperswithcode.com/task/3d-object-detection"
  },
  {
    "objectID": "research_object_segmentation.html#papers",
    "href": "research_object_segmentation.html#papers",
    "title": "Research Object Segmentation and Object detection",
    "section": "",
    "text": "(pdf) High-quality indoor scene 3D reconstruction with RGB-D cameras: A brief review - https://link.springer.com/article/10.1007/s41095-021-0250-8\n(pdf) Deep Learning for Generic Object Detection: A Survey - https://link.springer.com/article/10.1007/s11263-019-01247-4\nProgress in multi-object detection models: a comprehensive survey - https://link.springer.com/article/10.1007/s11042-022-14131-0\nDepth-Assisted Joint Detection Network For Monocular 3d Object Detection - https://ieeexplore.ieee.org/document/9506647\nMonocular 3D Object Detection of Moving Objects Using Random Sampling and Deep Layer Aggregation - https://ieeexplore.ieee.org/document/10043422\nDepth-Assisted Joint Detection Network For Monocular 3d Object Detection - https://ieeexplore.ieee.org/document/9966379\nCross-Modality Knowledge Distillation Network for Monocular 3D Object Detection - https://arxiv.org/pdf/2211.07171v1.pdf\n3D Object Detection - https://paperswithcode.com/task/3d-object-detection"
  },
  {
    "objectID": "research_object_segmentation.html#timeline",
    "href": "research_object_segmentation.html#timeline",
    "title": "Research Object Segmentation and Object detection",
    "section": "Timeline",
    "text": "Timeline"
  },
  {
    "objectID": "research_object_segmentation.html#datasets",
    "href": "research_object_segmentation.html#datasets",
    "title": "Research Object Segmentation and Object detection",
    "section": "Datasets",
    "text": "Datasets\nnuScenes\nkitti\nImagenet -"
  },
  {
    "objectID": "research_object_segmentation.html#models",
    "href": "research_object_segmentation.html#models",
    "title": "Research Object Segmentation and Object detection",
    "section": "Models",
    "text": "Models\ndetectron2 - https://ai.meta.com/tools/detectron2/\nYolo"
  }
]